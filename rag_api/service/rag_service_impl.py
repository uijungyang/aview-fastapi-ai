import os
from dotenv import load_dotenv
from openai import OpenAI, AsyncOpenAI
from chroma.client import client
from rag_api.entity.embedding import get_embedding
from rag_api.repository.vector_repository_impl import RagVectorRepositoryImpl

# load_dotenv()
#client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

class RagServiceImpl:

    def __init__(self):
        self.ragVectorRepository = RagVectorRepositoryImpl()
        self.client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    def summarize_metadata_from_collection(self, collection) -> str:
        result = collection.get()

        job_categories = []
        question_types = []
        tags = []

        for metadata in result["metadatas"]:
            job_categories.append(metadata.get("jobCategory", ""))
            question_types.append(metadata.get("questionType", ""))
            tags.append(metadata.get("tag", ""))

        def most_common(lst):
            return max(set(lst), key=lst.count) if lst else "ì •ë³´ ì—†ìŒ"

        return (
            f"- ìì£¼ ë‚˜ì˜¤ëŠ” ì§ë¬´ ë¶„ì•¼: {most_common(job_categories)}\n"
            f"- ì§ˆë¬¸ ìœ í˜•: {most_common(question_types)}\n"
            f"- íƒœê·¸ í‚¤ì›Œë“œ: {most_common(tags)}"
        )

    async def generate_interview_question(self, company: str, situation: str) -> dict:
        print("ğŸ”¥ğŸ”¥ğŸ”¥ RAG ì§„ì… í™•ì¸!")
        print(f"[RAG] Generating question using RAG... (company={company})")

        # 1. ê¸°ì—… ì»¬ë ‰ì…˜ ë° ë©”íƒ€ë°ì´í„° ìš”ì•½
        collection = self.ragVectorRepository.get_collection(company)
        metadata_summary = self.summarize_metadata_from_collection(collection)

        # 2. ìƒí™© ì„¤ëª… ì„ë² ë”© í›„ ê°€ì¥ ìœ ì‚¬í•œ ê¸°ì¡´ ì§ˆë¬¸ ì¶”ì¶œ
        embedding = get_embedding(situation)
        result = collection.query(query_embeddings=[embedding], n_results=3)
        documents = result["documents"][0] if result["documents"] else []
        context = "\n".join(documents) if documents else (
            "ìœ ì‚¬ ì§ˆë¬¸ì€ ì—†ì§€ë§Œ, ì‚¬ìš©ìì˜ ë‹µë³€ê³¼ ê¸°ì—…ì˜ ìŠ¤íƒ€ì¼ì„ ê³ ë ¤í•´ ìì—°ìŠ¤ëŸ¬ìš´ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•´ ì£¼ì„¸ìš”."
        )

        # 3. GPT í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""
        ë‹¹ì‹ ì€ {company}ì˜ AI ë©´ì ‘ê´€ì´ì•¼.
        ì´ íšŒì‚¬ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ìŠ¤íƒ€ì¼ì˜ ì§ˆë¬¸ì„ í•´: {metadata_summary}
        
        í›„ë³´ìì˜ ë°°ê²½ ë˜ëŠ” ì¸í„°ë·° ìƒí™©:{situation}
        
        ê³¼ê±°ì— ì‹¤ì œë¡œ ë¬¼ì–´ë³¸ ìœ ì‚¬ ì§ˆë¬¸ ì˜ˆì‹œ: {context}
        
        ì´ íšŒì‚¬ì˜ ìŠ¤íƒ€ì¼ê³¼ ìƒí™©ì— ë§ëŠ” ìƒˆë¡œìš´ í˜„ì‹¤ì ì¸ ë©´ì ‘ ì§ˆë¬¸ì„ í•˜ë‚˜ ìƒì„±í•´ì¤˜.
        ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì§ˆë¬¸ í•œ ë¬¸ì¥ë§Œ ì¶œë ¥í•´.
        """


        # 4. GPT í˜¸ì¶œ
        response = await self.client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )

        return {
            "generated_question": response.choices[0].message.content.strip(),
            "used_context": context,
            "summary": metadata_summary
        }