import os
from dotenv import load_dotenv
from openai import OpenAI, AsyncOpenAI
from chroma.client import client
from rag_api.entity.embedding import get_embedding
from rag_api.repository.vector_repository_impl import RagVectorRepositoryImpl

# load_dotenv()
#client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

class RagServiceImpl:

    def __init__(self):
        self.ragVectorRepository = RagVectorRepositoryImpl()
        self.client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    def summarize_metadata_from_collection(self, collection) -> str:
        result = collection.get()

        job_categories = []
        question_types = []
        tags = []

        for metadata in result["metadatas"]:
            job_categories.append(metadata.get("jobCategory", ""))
            question_types.append(metadata.get("questionType", ""))
            tags.append(metadata.get("tag", ""))

        def most_common(lst):
            return max(set(lst), key=lst.count) if lst else "ì •ë³´ ì—†ìŒ"

        return (
            f"- ìì£¼ ë‚˜ì˜¤ëŠ” ì§ë¬´ ë¶„ì•¼: {most_common(job_categories)}\n"
            f"- ì§ˆë¬¸ ìœ í˜•: {most_common(question_types)}\n"
            f"- íƒœê·¸ í‚¤ì›Œë“œ: {most_common(tags)}"
        )

    async def generate_interview_question(self, company: str, jobCategory: str, situation: str) -> dict:
        print("ğŸ”¥ğŸ”¥ğŸ”¥ RAG ì§„ì… í™•ì¸!")
        print(f"[RAG] Generating question using RAG... (company={company})")


        # 1. ê¸°ì—… ì»¬ë ‰ì…˜ ë° ë©”íƒ€ë°ì´í„° ìš”ì•½
        collection = self.ragVectorRepository.get_collection(company)
        metadata_summary = self.summarize_metadata_from_collection(collection)
        # ì§€ê¸ˆ ì´ ë©”íƒ€ë°ì´í„° summaryë¥¼ ê°€ì ¸ì™€ë„ í”„ë¡¬í”„íŠ¸ì—ì„œ í™œìš©ì„ ëª»í•˜ëŠ” ìƒí™©


        # 2. ìƒí™© ì„¤ëª… ì„ë² ë”© í›„ ê°€ì¥ ìœ ì‚¬í•œ ê¸°ì¡´ ì§ˆë¬¸ ì¶”ì¶œ
        embedding = get_embedding(situation)
        result = collection.query(query_embeddings=[embedding], n_results=3)
        documents = result["documents"][0] if result["documents"] else []
        context = "\n".join(documents) if documents else (
            "ìœ ì‚¬ ì§ˆë¬¸ì€ ì—†ì§€ë§Œ, ì‚¬ìš©ìì˜ ë‹µë³€ê³¼ ê¸°ì—…ì˜ ìŠ¤íƒ€ì¼ì„ ê³ ë ¤í•´ ìì—°ìŠ¤ëŸ¬ìš´ ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•´ ì£¼ì„¸ìš”."
        )

        requirements = self.ragVectorRepository.get_requirements(company, jobCategory)

        # 3. GPT í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        # question -> situation: ë©´ì ‘ìê°€ ì´ì „ì— í•œ ë‹µë³€(ë˜ëŠ” ë‹µë³€ ìš”ì•½)ì„ í•œ ì¤„ë¡œ ë³€ê²½í•´ì„œ ì´ìª½ ì½”ë“œë¡œ ë³´ë‚¸ê²ƒì„
        # ì´í›„ ê·¸ê±¸ ê¸°ë°˜ìœ¼ë¡œ GPTê°€ "ê·¸ëŸ¼ ì´ëŸ° ìƒí™©ì´ë©´ ì´ëŸ° ì§ˆë¬¸ì´ ì ì ˆí•˜ê² ë‹¤" í•˜ê³  ìƒˆ ì§ˆë¬¸ì„ ë§Œë“¤ì–´ë‚´ëŠ” ìš©ë„

        prompt = f"""
        ë‹¹ì‹ ì€ {company}ì˜ {jobCategory} ê¸°ìˆ  ë©´ì ‘ê´€ì´ì•¼.
        ë‹¤ìŒì€ íšŒì‚¬ì˜ ìš”êµ¬ì‚¬í•­ì´ì•¼: {requirements}

        ìƒí™©: "{situation}"
        ê³¼ê±° ìœ ì‚¬ ì§ˆë¬¸ ì˜ˆì‹œ:{context}

        ìœ„ ë‚´ìš©ì„ ì°¸ê³ í•´ ì ì ˆí•œ ê¸°ìˆ  ë©´ì ‘ ì§ˆë¬¸ì„ ìƒì„±í•´ì¤˜.
        ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì§ˆë¬¸ í•œ ë¬¸ì¥ë§Œ ì¶œë ¥í•´.
        """


        # 4. GPT í˜¸ì¶œ
        response = await self.client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )

        return {
            "generated_question": response.choices[0].message.content.strip(),
            "used_context": context,
            "summary": metadata_summary
        }

