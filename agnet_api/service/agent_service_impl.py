import os
from dotenv import load_dotenv
#from langchain.chat_models import ChatOpenAI
from langchain_openai import ChatOpenAI
from agnet_api.repository.agent_repository_impl import AgentRepositoryImpl
from agnet_api.repository.rag_repository_impl import RagRepositoryImpl

load_dotenv()

class AgentServiceImpl:
    def __init__(self):
        self.agentRepository = AgentRepositoryImpl()
        self.ragRepository = RagRepositoryImpl()
        self.openAPI = ChatOpenAI(api_key=os.getenv("OPENAI_API_KEY"), temperature=0)

    async def get_best_followup_question(self, companyName: str, topic: str, situation: str, gpt_question: str):
        # situation : answerText (ì´ì „ ì§ˆë¬¸ì— ëŒ€í•œ ë©´ì ‘ìì˜ ë‹µë³€)
        print(f"ğŸ”¥ AGENT started: company={companyName}, topic={topic}")

        # 1. RAG 1ì°¨ (ë©”ì¸ íšŒì‚¬ DB)
        rag_main_result = self.ragRepository.rag_main(companyName, situation)
        print(f"ğŸŸ¢ RAG Main ê²°ê³¼: {rag_main_result}")

        # 2. RAG 2ì°¨ (Fallback DB) ì¡°ê±´ë¶€ í˜¸ì¶œ
        rag_fallback_result = []
        if not rag_main_result:
            print(f"ğŸ”„ RAG Main ì‹¤íŒ¨ â†’ Fallback DB ì¡°íšŒ ì§„í–‰")
            rag_fallback_result = self.ragRepository.rag_fallback(situation)
            print(f"ğŸŸ¢ RAG Fallback ê²°ê³¼: {rag_fallback_result}")

        # 3. AGENTì—ê²Œ ìµœì¢… ì„ íƒ ìš”ì²­
        decision_prompt = self.agentRepository.build_decision_prompt(
            companyName, topic, gpt_question, rag_main_result, rag_fallback_result
        )

        print(f"ğŸ“ AGENT Prompt:\n{decision_prompt}")

        response = self.openAPI .predict(decision_prompt)
        print(f"ğŸ¯ AGENT ìµœì¢… ì„ íƒ: {response}")

        # 4. used_context / summary ë¦¬í„´ í¬ë§·
        used_context = "\n".join(rag_main_result or rag_fallback_result)
        summary = f"{companyName} DB ê²€ìƒ‰ + Fallback ì—¬ë¶€ í¬í•¨"

        return response, used_context, summary
